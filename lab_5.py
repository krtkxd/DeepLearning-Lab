# -*- coding: utf-8 -*-
"""LAB-5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14lqrO5s5X1wH2i7eoTim346Z0CGr2UtE
"""

from google.colab import files

# This will open a file picker dialog box
uploaded = files.upload()

# Get the filename of your uploaded model
for filename in uploaded.keys():
  print(f'User uploaded file "{filename}"')

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import pandas as pd

# Assuming the uploaded file is a CSV, load it into a pandas DataFrame
# The filename is stored in the `uploaded` dictionary keys
filename = next(iter(uploaded.keys()))
df = pd.read_csv(filename)

# Display the first 5 rows of the DataFrame
display(df.head())

# ----------------------------------------------------------
# (a) Activation Functions
# ----------------------------------------------------------

x = np.linspace(-10, 10, 200)

def sigmoid(x): return 1 / (1 + np.exp(-x))
def relu(x): return np.maximum(0, x)
def tanh(x): return np.tanh(x)
def softmax(x):
    exps = np.exp(x - np.max(x))
    return exps / np.sum(exps)

plt.figure(figsize=(12, 8))
plt.subplot(2,2,1); plt.plot(x, sigmoid(x)); plt.title("Sigmoid")
plt.subplot(2,2,2); plt.plot(x, relu(x)); plt.title("ReLU")
plt.subplot(2,2,3); plt.plot(x, tanh(x)); plt.title("Tanh")

# Example of Softmax output for a sample input array
sample_input = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
softmax_output = softmax(sample_input)
plt.subplot(2,2,4); plt.bar(range(len(sample_input)), softmax_output); plt.title("Softmax Output (example)")
plt.xticks(range(len(sample_input)), sample_input)
plt.ylabel("Probability")

plt.show()

# ----------------------------------------------------------
# (b) Loss Functions
# ----------------------------------------------------------

y_true = np.array([0, 0, 1, 1])
y_pred_probs = np.linspace(0.01, 0.99, 100)

# MSE Loss
mse = [(yt - yp)**2 for yt in [0,1] for yp in y_pred_probs]
mse = np.array(mse).reshape(2, -1)

# Cross Entropy Loss
eps = 1e-9
cross_entropy = [-(yt*np.log(yp+eps)+(1-yt)*np.log(1-yp+eps)) for yt in [0,1] for yp in y_pred_probs]
cross_entropy = np.array(cross_entropy).reshape(2, -1)

plt.figure(figsize=(12, 5))
plt.subplot(1,2,1);
plt.plot(y_pred_probs, mse[0], label="True=0")
plt.plot(y_pred_probs, mse[1], label="True=1")
plt.title("MSE Loss"); plt.legend()

plt.subplot(1,2,2);
plt.plot(y_pred_probs, cross_entropy[0], label="True=0")
plt.plot(y_pred_probs, cross_entropy[1], label="True=1")
plt.title("Cross-Entropy Loss"); plt.legend()
plt.show()

# ----------------------------------------------------------
# (c) Backpropagation Implementation
# ----------------------------------------------------------

# Simple 2-layer Neural Network (manual backprop with NumPy)
np.random.seed(42)
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])   # XOR

# Initialize weights
W1 = np.random.randn(2, 4)
b1 = np.zeros((1,4))
W2 = np.random.randn(4, 1)
b2 = np.zeros((1,1))

lr = 0.1
epochs = 5000
losses = []

for epoch in range(epochs):
    # Forward
    z1 = X @ W1 + b1
    a1 = np.tanh(z1)
    z2 = a1 @ W2 + b2
    y_pred = 1/(1+np.exp(-z2))  # sigmoid

    # Loss (MSE)
    loss = np.mean((y - y_pred)**2)
    losses.append(loss)

    # Backprop
    dloss = 2*(y_pred - y)/y.shape[0]
    dz2 = dloss * y_pred*(1-y_pred)
    dW2 = a1.T @ dz2
    db2 = np.sum(dz2, axis=0, keepdims=True)

    da1 = dz2 @ W2.T
    dz1 = da1 * (1-np.tanh(z1)**2)
    dW1 = X.T @ dz1
    db1 = np.sum(dz1, axis=0, keepdims=True)

    # Update
    W1 -= lr*dW1
    b1 -= lr*db1
    W2 -= lr*dW2
    b2 -= lr*db2

plt.plot(losses)
plt.title("Backpropagation Training Loss (XOR)")
plt.show()

# ----------------------------------------------------------
# (d) Optimizers Comparison (PyTorch)
# ----------------------------------------------------------

# Dataset (binary classification)
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)
X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)

class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2, 16)
        self.fc2 = nn.Linear(16, 2)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

def train_model(optimizer_name):
    model = SimpleNN()
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=0.01)
    elif optimizer_name == "Momentum":
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    elif optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=0.01)

    losses = []
    for epoch in range(100):
        optimizer.zero_grad()
        out = model(X_train)
        loss = criterion(out, y_train)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    return losses

optimizers = ["SGD", "Momentum", "Adam"]
plt.figure(figsize=(10,6))
for opt in optimizers:
    losses = train_model(opt)
    plt.plot(losses, label=opt)
plt.title("Optimizer Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# (a) Implement and Visualize Activation Functions on BMW Sales Dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset (assuming you already uploaded in Colab)
df = pd.read_csv("BMW sales data (2010-2024) (1).csv")

# Select a numeric column (e.g., sales) for visualization
# If column names differ, replace "Sales" with the correct one
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("Numeric columns:", numeric_cols)

# Take the first numeric column for demo
x = df[numeric_cols[0]].values.astype(float)
x = (x - np.mean(x)) / np.std(x)   # normalize for better visualization

# Define Activation Functions
def sigmoid(z): return 1 / (1 + np.exp(-z))
def relu(z): return np.maximum(0, z)
def tanh(z): return np.tanh(z)
def softmax(z):
    exp = np.exp(z - np.max(z))
    return exp / np.sum(exp)

# Apply on dataset values
y_sigmoid = sigmoid(x)
y_relu = relu(x)
y_tanh = tanh(x)
y_softmax = softmax(x)

# Visualization
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(x, y_sigmoid, 'g')
plt.title("Sigmoid Activation")

plt.subplot(2, 2, 2)
plt.plot(x, y_relu, 'r')
plt.title("ReLU Activation")

plt.subplot(2, 2, 3)
plt.plot(x, y_tanh, 'b')
plt.title("Tanh Activation")

plt.subplot(2, 2, 4)
plt.plot(x, y_softmax, 'm')
plt.title("Softmax Activation")

plt.tight_layout()
plt.show()

# (b) Implement and Visualize Loss Functions: MSE, Cross-Entropy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("BMW sales data (2010-2024) (1).csv")

# Pick first numeric column (e.g., Sales)
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("Numeric columns:", numeric_cols)

sales = df[numeric_cols[0]].values.astype(float)

# Normalize for stability
sales = (sales - np.min(sales)) / (np.max(sales) - np.min(sales))

# --------------------------------------------------------
# 1. MSE Loss (Regression Example)
# --------------------------------------------------------
# True values
y_true = sales

# Simulated predicted values (slight variation around true)
y_pred = y_true + np.random.normal(0, 0.1, size=len(y_true))

# Mean Squared Error function
def mse_loss(y, y_hat):
    return np.mean((y - y_hat) ** 2)

mse_values = []
errors = np.linspace(-1, 1, 100)
for e in errors:
    y_pred_shifted = y_true + e
    mse_values.append(mse_loss(y_true, y_pred_shifted))

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(errors, mse_values, 'b')
plt.title("MSE Loss Behavior")
plt.xlabel("Prediction Shift")
plt.ylabel("MSE")

# --------------------------------------------------------
# 2. Cross-Entropy Loss (Classification Example)
# --------------------------------------------------------
# Create a binary target: High Sales (1) if above median else Low Sales (0)
threshold = np.median(sales)
y_class = (sales > threshold).astype(int)

# Predicted probabilities from 0.01 to 0.99
y_pred_probs = np.linspace(0.01, 0.99, 100)

# Cross-Entropy function
eps = 1e-9
def cross_entropy(y, y_hat):
    return -(y*np.log(y_hat+eps) + (1-y)*np.log(1-y_hat+eps))

# Compute CE for True=0 and True=1
ce_0 = [cross_entropy(0, p) for p in y_pred_probs]
ce_1 = [cross_entropy(1, p) for p in y_pred_probs]

plt.subplot(1,2,2)
plt.plot(y_pred_probs, ce_0, label="True = 0 (Low Sales)")
plt.plot(y_pred_probs, ce_1, label="True = 1 (High Sales)")
plt.title("Cross-Entropy Loss Behavior")
plt.xlabel("Predicted Probability")
plt.ylabel("Cross-Entropy Loss")
plt.legend()

plt.tight_layout()
plt.show()

# (c) Implement Backpropagation for training the network (on BMW Sales Data)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("BMW sales data (2010-2024) (1).csv")

# Select first numeric column (e.g., Sales)
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("Numeric columns:", numeric_cols)

sales = df[numeric_cols[0]].values.astype(float)

# Normalize inputs (X = year, y = sales normalized)
X = np.arange(len(sales)).reshape(-1,1)  # time as input (0,1,2,...)
y = (sales - np.min(sales)) / (np.max(sales) - np.min(sales))  # normalize to 0-1

# Initialize parameters for 1-hidden-layer NN
np.random.seed(42)
input_dim = 1
hidden_dim = 8
output_dim = 1

W1 = np.random.randn(input_dim, hidden_dim) * 0.1
b1 = np.zeros((1, hidden_dim))
W2 = np.random.randn(hidden_dim, output_dim) * 0.1
b2 = np.zeros((1, output_dim))

# Activation function (ReLU) and its derivative
def relu(z): return np.maximum(0, z)
def relu_deriv(z): return (z > 0).astype(float)

# Loss function (MSE)
def mse_loss(y, y_hat):
    return np.mean((y - y_hat)**2)

# Training with Backpropagation
lr = 0.01
epochs = 5000
losses = []

for epoch in range(epochs):
    # Forward pass
    z1 = X @ W1 + b1
    a1 = relu(z1)
    z2 = a1 @ W2 + b2
    y_pred = z2  # Linear output for regression

    # Compute loss
    loss = mse_loss(y, y_pred)
    losses.append(loss)

    # Backpropagation
    dloss = 2 * (y_pred - y) / len(y)   # dL/dy_pred

    dW2 = a1.T @ dloss
    db2 = np.sum(dloss, axis=0, keepdims=True)

    da1 = dloss @ W2.T
    dz1 = da1 * relu_deriv(z1)
    dW1 = X.T @ dz1
    db1 = np.sum(dz1, axis=0, keepdims=True)

    # Update weights
    W1 -= lr * dW1
    b1 -= lr * db1
    W2 -= lr * dW2
    b2 -= lr * db2

# Plot training loss
plt.plot(losses)
plt.title("Training Loss (Backpropagation on BMW Sales Data)")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.show()

# Predictions vs Actual
plt.figure(figsize=(8,5))
plt.plot(y, label="Actual Sales (Normalized)")
plt.plot(y_pred, label="Predicted Sales")
plt.title("BMW Sales Prediction using Manual Backpropagation NN")
plt.legend()
plt.show()

# (d) Compare Optimizers (SGD, Momentum, Adam) on a small dataset

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# -----------------------------------------------------
# 1. Create a small dataset (binary classification)
# -----------------------------------------------------
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# -----------------------------------------------------
# 2. Define simple Neural Network
# -----------------------------------------------------
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 16)
        self.fc2 = nn.Linear(16, 2)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# -----------------------------------------------------
# 3. Training function for a given optimizer
# -----------------------------------------------------
def train_model(optimizer_name):
    model = SimpleNN()
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=0.01)
    elif optimizer_name == "Momentum":
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    elif optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=0.01)

    losses = []
    for epoch in range(100):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    return losses

# -----------------------------------------------------
# 4. Compare Optimizers
# -----------------------------------------------------
optimizers = ["SGD", "Momentum", "Adam"]

plt.figure(figsize=(10,6))
for opt in optimizers:
    losses = train_model(opt)
    plt.plot(losses, label=opt)

plt.title("Optimizer Comparison on Small Dataset")
plt.xlabel("Epochs")
plt.ylabel("Training Loss")
plt.legend()
plt.show()