# -*- coding: utf-8 -*-
"""kiramda lab 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NeZHqVsYnGxhH07qOZgXGyhayZntCS-K
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Load the uploaded CSV into a DataFrame
df = pd.read_csv("Salesforce_stock_history edited.csv")

print("First 5 rows:")
print(df.head())
print("\nData Info:")
print(df.info())

# Convert 'Date' column to datetime and handle missing values
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df.dropna(subset=['Date'], inplace=True)

# 2. Exploratory Data Analysis (EDA)
plt.figure(figsize=(10,5))
plt.plot(df['Date'], df['Close'])
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('Salesforce Closing Price Over Time')
plt.xticks(rotation=45)
plt.show()

# Correlation heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# 3. Data Preprocessing / Feature Engineering
# Target: 1 if next day's Close > today's Close, else 0
df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)
df = df.dropna()

X = df[['Open','High','Low','Close','Volume']].values
y = df['Target'].values

# Stratified split to keep class balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Torch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# DataLoader for batching
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 4. Build Neural Network Model
class StockNN(nn.Module):
    def __init__(self):
        super(StockNN, self).__init__()
        self.fc1 = nn.Linear(5, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.fc4(x)
        return x

model = StockNN()

# Loss & optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)

# 5. Train the Model
epochs = 50
train_losses, test_losses, acc_list = [], [], []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    # Validation
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        test_loss = criterion(test_outputs, y_test_tensor).item()
        _, preds = torch.max(test_outputs, 1)
        acc = accuracy_score(y_test, preds.numpy())

    scheduler.step()

    train_losses.append(running_loss/len(train_loader))
    test_losses.append(test_loss)
    acc_list.append(acc)

    print(f"Epoch {epoch+1}/{epochs}, "
          f"Train Loss: {running_loss/len(train_loader):.4f}, "
          f"Test Loss: {test_loss:.4f}, "
          f"Acc: {acc:.4f}")


# 6. Plot Loss and Accuracy
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')

plt.subplot(1,2,2)
plt.plot(acc_list, label='Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Curve')

plt.show()

# 7. Confusion Matrix & Classification Report
model.eval()
with torch.no_grad():
    final_outputs = model(X_test_tensor)
    _, final_preds = torch.max(final_outputs, 1)

cm = confusion_matrix(y_test, final_preds.numpy())
ConfusionMatrixDisplay(cm).plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, final_preds.numpy()))

#Additional performance metrics
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

acc = accuracy_score(y_test, final_preds.numpy())
precision = precision_score(y_test, final_preds.numpy())
recall = recall_score(y_test, final_preds.numpy())
f1 = f1_score(y_test, final_preds.numpy())

print("\nPerformance Metrics:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, final_preds.numpy()))

import matplotlib.pyplot as plt

def draw_neural_net(layer_sizes):
    """
    Draw a simple feed-forward neural network diagram.
    layer_sizes: list with number of neurons per layer
    Example: [5, 128, 64, 32, 2]
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.axis('off')

    v_spacing = 1
    h_spacing = 2

    # Calculate center positions
    n_layers = len(layer_sizes)
    neuron_positions = []

    for i, layer_size in enumerate(layer_sizes):
        x = i * h_spacing
        y_start = -(layer_size - 1) * v_spacing / 2
        positions = []
        for j in range(layer_size):
            y = y_start + j * v_spacing
            positions.append((x, y))
            circle = plt.Circle((x, y), 0.2, color='skyblue', ec='k', zorder=4)
            ax.add_artist(circle)
        neuron_positions.append(positions)

    # Draw connections
    for i in range(n_layers - 1):
        for (x1, y1) in neuron_positions[i]:
            for (x2, y2) in neuron_positions[i+1]:
                line = plt.Line2D([x1, x2], [y1, y2], c='gray')
                ax.add_artist(line)

    # Layer labels
    for i, size in enumerate(layer_sizes):
        ax.text(i * h_spacing, max(p[1] for p in neuron_positions[i]) + 0.5,
                f"Layer {i+1}\n({size} neurons)",
                ha='center', fontsize=10, fontweight='bold')

    plt.title("Neural Network Architecture", fontsize=14, fontweight='bold')
    plt.show()

import matplotlib.pyplot as plt

def draw_neural_net_inline(layer_sizes):
    """
    Draw a feed-forward neural network diagram inline (in Colab/Notebook).
    """
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.axis('off')

    v_spacing = 0.6  # vertical spacing between neurons
    h_spacing = 1.5  # horizontal spacing between layers

    n_layers = len(layer_sizes)
    neuron_positions = []

    for i, layer_size in enumerate(layer_sizes):
        # Limit neurons drawn per layer for readability
        draw_neurons = min(layer_size, 10)
        x = i * h_spacing
        y_start = -(draw_neurons - 1) * v_spacing / 2
        positions = []
        for j in range(draw_neurons):
            y = y_start + j * v_spacing
            positions.append((x, y))
            circle = plt.Circle((x, y), 0.15, color='skyblue', ec='k', zorder=4)
            ax.add_artist(circle)
        neuron_positions.append(positions)

    # Draw connections
    for i in range(n_layers - 1):
        for (x1, y1) in neuron_positions[i]:
            for (x2, y2) in neuron_positions[i+1]:
                line = plt.Line2D([x1, x2], [y1, y2], c='gray', linewidth=0.4)
                ax.add_artist(line)

    # Layer labels
    for i, size in enumerate(layer_sizes):
        ax.text(i * h_spacing, max(p[1] for p in neuron_positions[i]) + 0.5,
                f"Layer {i+1}\n({size} neurons)",
                ha='center', fontsize=8, fontweight='bold')

    plt.title("Neural Network Architecture", fontsize=12, fontweight='bold')
    plt.show()

# Example: StockNN architecture
draw_neural_net_inline([5, 128, 64, 32, 2])

import matplotlib.pyplot as plt

def draw_neural_net_inline(layer_sizes):
    """
    Draw a feed-forward neural network diagram inline (in Colab/Notebook).
    Includes Input, Hidden, and Output layer labels.
    """
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.axis('off')

    v_spacing = 0.6  # vertical spacing between neurons
    h_spacing = 1.8  # horizontal spacing between layers

    n_layers = len(layer_sizes)
    neuron_positions = []

    for i, layer_size in enumerate(layer_sizes):
        # Limit neurons drawn per layer for readability
        draw_neurons = min(layer_size, 10)
        x = i * h_spacing
        y_start = -(draw_neurons - 1) * v_spacing / 2
        positions = []
        for j in range(draw_neurons):
            y = y_start + j * v_spacing
            positions.append((x, y))
            circle = plt.Circle((x, y), 0.15, color='skyblue', ec='k', zorder=4)
            ax.add_artist(circle)
        neuron_positions.append(positions)

    # Draw connections
    for i in range(n_layers - 1):
        for (x1, y1) in neuron_positions[i]:
            for (x2, y2) in neuron_positions[i+1]:
                line = plt.Line2D([x1, x2], [y1, y2], c='gray', linewidth=0.4)
                ax.add_artist(line)

    # Layer labels
    for i, size in enumerate(layer_sizes):
        if i == 0:
            label = f"Input Layer\n({size} features)"
        elif i == n_layers - 1:
            label = f"Output Layer\n({size} classes)"
        else:
            label = f"Hidden Layer {i}\n({size} neurons)"

        ax.text(i * h_spacing, max(p[1] for p in neuron_positions[i]) + 0.5,
                label, ha='center', fontsize=9, fontweight='bold')

    plt.title("Neural Network Architecture", fontsize=12, fontweight='bold')
    plt.show()

# Example: StockNN architecture (Input: 5, Hidden: 128-64-32, Output: 2)
draw_neural_net_inline([5, 128, 64, 32, 2])